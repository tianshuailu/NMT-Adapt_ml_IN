{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tianshuailu/NMT-Adapt_ml_IN/blob/main/scripts/Pretrain_mBART.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Downloading mbart.cc25**"
      ],
      "metadata": {
        "id": "ioxMS_lUeknU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://dl.fbaipublicfiles.com/fairseq/models/mbart/mbart.cc25.v2.tar.gz \n",
        "!tar -xzvf mbart.cc25.v2.tar.gz"
      ],
      "metadata": {
        "id": "Agw5r9BTeoti"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Byte-Pair Encoding**\n",
        "\n",
        "We used the sentencepiece model coming with the mbart.cc25 model to do the Byte-Pair Encoding on Hindi and English parallel data"
      ],
      "metadata": {
        "id": "jXi3F02ee-tk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cat train.en_XX | spm_encode --model /your_path/sentence.bpe.model > train.spm.en_XX"
      ],
      "metadata": {
        "id": "R-lfUN-ieGJG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Generating dictionary**\n",
        "\n",
        "The python snippet build_vocab.py for generating vocabulary is from this GitHub issue https://github.com/facebookresearch/fairseq/issues/2120"
      ],
      "metadata": {
        "id": "DXjVsvRG5mKC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python build_vocab.py --corpus-data \"/your_path/data\" --langs ar_AR,cs_CZ,de_DE,en_XX,es_XX,et_EE,fi_FI,fr_XX,gu_IN,hi_IN,it_IT,ja_XX,kk_KZ,ko_KR,lt_LT,lv_LV,my_MM,ne_NP,nl_XX,ro_RO,ru_RU,si_LK,tr_TR,vi_VN,zh_CN --output ./dict.txt"
      ],
      "metadata": {
        "id": "dGz3myOjGMlT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Triming mBART**\n",
        "\n",
        "The python snippet trim_mbart.py for trimming mBART is from this GitHub issue https://github.com/facebookresearch/fairseq/issues/2120"
      ],
      "metadata": {
        "id": "BMabjerz5zux"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# we changed the last three language label to ml_XX,no_ML,no_HI, refering to Malayalam, noised Malayalam, and noised Hindi\n",
        "!python trim_mbart.py --pre-train-dir /your_path/mbart.cc25.v2 --ft-dict /your_path/dict.txt --langs ar_AR,cs_CZ,de_DE,en_XX,es_XX,et_EE,fi_FI,fr_XX,gu_IN,hi_IN,it_IT,ja_XX,kk_KZ,ko_KR,lt_LT,lv_LV,my_MM,ne_NP,nl_XX,ro_RO,ru_RU,si_LK,ml_XX,no_ML,no_HI --output /your_path/model.pt"
      ],
      "metadata": {
        "id": "cUTw-gT0XcHO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Preprocessing data with fairseq-preprocess**"
      ],
      "metadata": {
        "id": "EhlBqYZ_-D0p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# we used fairseq-preprocess to preprocess the data, srcdict and tgtdict should be the same dictionary file\n",
        "!fairseq-preprocess \\\n",
        "--source-lang hi_IN \\\n",
        "--target-lang en_XX \\\n",
        "--trainpref /your_path/train.spm \\\n",
        "--validpref /your_path/valid.spm \\\n",
        "--testpref /your_path/test.spm \\\n",
        "--destdir /your_path/pp_data \\\n",
        "--thresholdtgt 0 \\\n",
        "--thresholdsrc 0 \\\n",
        "--srcdict /your_path/dict.txt \\\n",
        "--tgtdict /your_path/dict.txt \\\n",
        "--workers 70"
      ],
      "metadata": {
        "id": "Jk7yfCLzkZ-2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Pretraining mBART**"
      ],
      "metadata": {
        "id": "nIRke4Ua45_u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# we used fairseq-train to train the model\n",
        "!fairseq-train /your_path/preprocessed_data \\\n",
        "--encoder-normalize-before --decoder-normalize-before \\\n",
        "--arch mbart_large --layernorm-embedding \\\n",
        "--task translation_from_pretrained_bart \\\n",
        "--source-lang hi_IN --target-lang en_XX \\\n",
        "--criterion label_smoothed_cross_entropy --label-smoothing 0.2 \\\n",
        "--optimizer adam --adam-eps 1e-06 --adam-betas '(0.9, 0.98)' \\\n",
        "--lr-scheduler polynomial_decay --lr 3e-05 --min-lr -1 --warmup-updates 2500 --total-num-update 40000 \\\n",
        "--dropout 0.3 --attention-dropout 0.1 --weight-decay 0.0 \\\n",
        "--max-tokens 1024 --update-freq 2 \\\n",
        "--save-interval 1 --save-interval-updates 5000 --keep-interval-updates 10 --no-epoch-checkpoints \\\n",
        "--seed 222 --log-format simple --log-interval 2 \\\n",
        "--save-dir /your_path/checkpoints \\\n",
        "--restore-file /your_path/pretrained_model.pt \\\n",
        "--reset-optimizer --reset-meters --reset-dataloader --reset-lr-scheduler \\\n",
        "--langs ar_AR,cs_CZ,de_DE,en_XX,es_XX,et_EE,fi_FI,fr_XX,gu_IN,hi_IN,it_IT,ja_XX,kk_KZ,ko_KR,lt_LT,lv_LV,my_MM,ne_NP,nl_XX,ro_RO,ru_RU,si_LK,ml_XX,no_ML,no_HI \\\n",
        "--ddp-backend no_c10d"
      ],
      "metadata": {
        "id": "8YDeBymjwEjJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Checkpoint Evaluation**"
      ],
      "metadata": {
        "id": "Sp8SNGk6dc-T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# we used the following command to check the translations and bleu score of the checkpoints\n",
        "!fairseq-generate /your_path/pp_data \\\n",
        "  --path /your_path/checkpoint_last.pt \\\n",
        "  --results-path /your_path/eval_result \\\n",
        "  --task translation_from_pretrained_bart \\\n",
        "  --gen-subset test \\\n",
        "  -t en_XX -s hi_IN \\\n",
        "  --bpe 'sentencepiece' --sentencepiece-model /your_path/spm.model \\\n",
        "  --scoring sacrebleu \\\n",
        "  --batch-size 32 --langs ar_AR,cs_CZ,de_DE,en_XX,es_XX,et_EE,fi_FI,fr_XX,gu_IN,hi_IN,it_IT,ja_XX,kk_KZ,ko_KR,lt_LT,lv_LV,my_MM,ne_NP,nl_XX,ro_RO,ru_RU,si_LK,ml_XX,no_ML,no_HI"
      ],
      "metadata": {
        "id": "wd3F8JuW708H"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}